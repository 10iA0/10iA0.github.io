<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1"><title>Horseman, pass by.</title><meta charset=utf-8><meta name=description content="Ladder@3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use ssh directelly, it will report Host key verification failed. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically."><meta name=author content="Jihang XIAO"><link rel=canonical href=https://10iA0.github.io/blog/hadoop-start-and-stop-cluster/><meta property="og:title" content="Hadoop-Start and stop cluste"><meta property="og:description" content="3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use ssh directelly, it will report Host key verification failed. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically."><meta property="og:type" content="article"><meta property="og:url" content="https://10iA0.github.io/blog/hadoop-start-and-stop-cluster/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2021-06-23T22:08:00+00:00"><meta property="article:modified_time" content="2021-06-23T22:08:00+00:00"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-appendix/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-configurate-a-cluster/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-running-a-cluster-write-sync-scripts/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-preparation-on-servers/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-appendix/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-configurate-a-cluster/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-running-a-cluster-write-sync-scripts/"><meta property="og:see_also" content="https://10iA0.github.io/blog/hadoop-preparation-on-servers/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hadoop-Start and stop cluste"><meta name=twitter:description content="3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use ssh directelly, it will report Host key verification failed. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://10iA0.github.io/blog/"},{"@type":"ListItem","position":3,"name":"Hadoop-Start and stop cluste","item":"https://10iA0.github.io/blog/hadoop-start-and-stop-cluster/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Hadoop-Start and stop cluste","name":"Hadoop-Start and stop cluste","description":"3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use ssh directelly, it will report Host key verification failed. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically.\n","keywords":["ssh","HDFS","YARN"],"articleBody":"3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use ssh directelly, it will report Host key verification failed. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically.\nConfigurate SSH I can login other sever by ssh [other server's IP]:\n[jihang@hadoop102 ~]$ ssh hadoop103 However, there would report an erro: host key verification failed.\nThe authenticity of host '192.168.1.103 (192.168.1.103)' can't be established. RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06. Are you sure you want to continue connecting (yes/no)? Though this connection could be established by printing yes, I want to configure ssh to login without password.\nCreate id_rsa and id_rsa.pub [jihang@hadoop102 .ssh]$ ssh-keygen -t rsa Explanation of files about SSH:\nfnames explanation id_rsa 密钥 id_rsa.pub 公钥 known_hosts 访问其它服务器，获得它们的公钥 authorized_keys 其它服务器访问，记录它们的公钥 Copy id_rsa.pub to servers Hadoop102 [jihang@hadoop102 .ssh]$ ssh-copy-id hadoop102 [jihang@hadoop102 .ssh]$ ssh-copy-id hadoop103 [jihang@hadoop102 .ssh]$ ssh-copy-id hadoop104 Hadoop103 [jihang@hadoop103 .ssh]$ ssh-copy-id hadoop102 [jihang@hadoop103 .ssh]$ ssh-copy-id hadoop103 [jihang@hadoop103 .ssh]$ ssh-copy-id hadoop104 Hadoop104 [jihang@hadoop104 .ssh]$ ssh-copy-id hadoop102 [jihang@hadoop104 .ssh]$ ssh-copy-id hadoop103 [jihang@hadoop104 .ssh]$ ssh-copy-id hadoop104 Configure workers When the start and stop shell scripts were ran, the file etc/hadoop/workers will be analyzed. The content on this files are address of all servers. These scripts would run HDFS and YARN automatically on servers.\n[jihang@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers edit content (do not leave space on content):\nhadoop102 hadoop103 hadoop104 Sync workers.\n[jihang@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/ Operate HDFS and YARN Attention! The NameNode should be formatted only and only if the HDFS is firstlly started.\nOnce the HDFS was started and NameNode need to be formatted again, a new cluster id would be created. Then the new cluster id on NameNode and old cluster id on DataNode would be different so that it would report an erro. Hence, if the NameNode need be formatted again, the processing of NameNode and DataNode should be stopped before. Besides, the dictionary data and logs should be deleted before.\nStart and stop HDFS start HDFS.\n[jihang@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh Enter http://hadoop102:9870 on web to check storage information on HDFS.\nstop HDFS.\n[jihang@hadoop102 hadoop-3.1.3]$ sbin/stop-dfs.sh Start and stop YARN Start YARN.\n[jihang@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh Enter http://hadoop103:8088 on web to check Jobs on YARN.\nstop Yarn.\n[jihang@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh Write and xsync shell scripts jps_cluster.sh Write a script that check Java’s processings on servers of a cluster: jps_cluster.sh\n[jihang@hadoop102 ~]$ cd /home/jihang/bin # [jihang@hadoop102 ~]$ touch jps_cluster.sh [jihang@hadoop102 ~]$ sudo vim jps_cluster.sh add content:\n#!/bin/bash for host in hadoop102 hadoop103 hadoop104 do echo =============== $host =============== ssh $host jps $@ | grep -v Jps done Authorize jps_cluster.sh.\n[jihang@hadoop102 bin]$ chmod +x jps_cluster Sync shell scripts.\n[jihang@hadoop102 ~]$ xsync /home/jihang/bin/ hadoop_cluster.sh Write a script that start and stop HDFS, YARN and historyserver on a cluster: hadoop_cluster.sh.\n[jihang@hadoop102 ~]$ cd /home/jihang/bin # [jihang@hadoop102 ~]$ touch hadoop_cluster.sh [jihang@hadoop102 ~]$ sudo vim hadoop_cluster.sh add content:\n#!/bin/bash if [ $# -lt 1 ] then echo \"No Args Input...\" exit ; fi case $1 in \"start\") echo \" =================== 启动 hadoop集群 ===================\" echo \" --------------- 启动 hdfs ---------------\" ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/start-dfs.sh\" echo \" --------------- 启动 yarn ---------------\" ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/start-yarn.sh\" echo \" --------------- 启动 historyserver ---------------\" ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver\" ;; \"stop\") echo \" =================== 关闭 hadoop集群 ===================\" echo \" --------------- 关闭 historyserver ---------------\" ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver\" echo \" --------------- 关闭 yarn ---------------\" ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh\" echo \" --------------- 关闭 hdfs ---------------\" ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh\" ;; *) echo \"Input Args Error...\" ;; esac Authorize myhdoop.sh.\n[jihang@hadoop102 bin]$ chmod +x hadoop_cluster.sh Sync shell scripts.\n[jihang@hadoop102 ~]$ xsync /home/jihang/bin/ Run example wordcount Run the official example wordcount on cluster.\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount file:///wcinput file:///wcoutput / in file:///wcinput and file:///wcoutput means root path in HDFS, which is configurated by core-site.xml:\nfs.defaultFS hdfs://hadoop102:9820 file:/// equals hdfs://hadoop102:9820/. So, The command below is equivalent to command above.\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount hdfs://hadoop102:9820/wcinput hdfs://hadoop102:9820/wcoutput ","wordCount":"675","inLanguage":"en","datePublished":"2021-06-23T22:08:00Z","dateModified":"2021-06-23T22:08:00Z","author":{"@type":"Person","name":"Jihang XIAO"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://10iA0.github.io/blog/hadoop-start-and-stop-cluster/"},"publisher":{"@type":"Organization","name":"Horseman, pass by.","logo":{"@type":"ImageObject","url":"https://10iA0.github.io/favicon.ico"}}}</script><link rel=icon type=image/png href=/images/sun.png sizes=16x16><link rel=apple-touch-icon href=/images/sun.png><link rel=manifest href=/images/sun.png><link rel=stylesheet href=/css/main.min.64bc8c4cb2304e84167c1583fa1b5de80f6d5adc95abed2e616dea0ea5680e01.css integrity="sha256-ZLyMTLIwToQWfBWD+htd6A9tWtyVq+0uYW3qDqVoDgE=" crossorigin=anonymous media=screen><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c098d85b5396dec4707ea2cead1445b4dc2ff0fc56b8dbbd9049d0d1c50ad237.js></script>
<script>hljs.highlightAll()</script><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/></a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/>Home</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/archives>Post</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/tags>Tags</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/about>About</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/10iA0><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://10iA0.github.io/zh/>中</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>Hadoop-Start and stop cluste</h1></header><p><small>June 23, 2021&nbsp;· 675 words&nbsp;· 4 min</small>
<small>·
<a href=https://10iA0.github.io/tags/ssh/>ssh</a>
<a href=https://10iA0.github.io/tags/hdfs/>HDFS</a>
<a href=https://10iA0.github.io/tags/yarn/>YARN</a></small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#configurate-ssh>Configurate SSH</a><ul><li><a href=#create-id_rsa-and-id_rsapub>Create id_rsa and id_rsa.pub</a></li><li><a href=#copy-id_rsapub-to-servers>Copy id_rsa.pub to servers</a></li></ul></li><li><a href=#configure-workers>Configure workers</a></li><li><a href=#operate-hdfs-and-yarn>Operate HDFS and YARN</a><ul><li><a href=#start-and-stop-hdfs>Start and stop HDFS</a></li><li><a href=#start-and-stop-yarn>Start and stop YARN</a></li></ul></li><li><a href=#write-and-xsync-shell-scripts>Write and <code>xsync</code> shell scripts</a><ul><li><a href=#jps_clustersh>jps_cluster.sh</a></li><li><a href=#hadoop_clustersh>hadoop_cluster.sh</a></li></ul></li><li><a href=#run-example-wordcount>Run example <code>wordcount</code></a></li></ul></nav></div><section class=blog-content><p>3 virtual machines were prepated, and JDK, Hadoop have installed and environmental variables have configurated on them. When I do big data computing, the servers shoud be communicated with each other. If I use <code>ssh</code> directelly, it will report <code>Host key verification failed</code>. So, I need make ssh login without password. Futhermore, I would like to start and stop cluster manually. So, I need write a script that could start and stop cluster automatically.</p><h2 id=configurate-ssh>Configurate SSH</h2><p>I can login other sever by <code>ssh [other server's IP]</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ ssh hadoop103
</span></span></code></pre></div><p>However, there would report an erro: <code>host key verification failed</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>The authenticity of host <span style=color:#e6db74>&#39;192.168.1.103 (192.168.1.103)&#39;</span> can<span style=color:#960050;background-color:#1e0010>&#39;</span>t be established.
</span></span><span style=display:flex><span>RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06.
</span></span><span style=display:flex><span>Are you sure you want to <span style=color:#66d9ef>continue</span> connecting <span style=color:#f92672>(</span>yes/no<span style=color:#f92672>)</span>? 
</span></span></code></pre></div><p>Though this connection could be established by printing <code>yes</code>, I want to configure ssh to login without password.</p><h3 id=create-id_rsa-and-id_rsapub>Create id_rsa and id_rsa.pub</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 .ssh<span style=color:#f92672>]</span>$ ssh-keygen -t rsa
</span></span></code></pre></div><p>Explanation of files about SSH:</p><table><thead><tr><th>fnames</th><th>explanation</th></tr></thead><tbody><tr><td>id_rsa</td><td>密钥</td></tr><tr><td>id_rsa.pub</td><td>公钥</td></tr><tr><td>known_hosts</td><td>访问其它服务器，获得它们的公钥</td></tr><tr><td>authorized_keys</td><td>其它服务器访问，记录它们的公钥</td></tr></tbody></table><h3 id=copy-id_rsapub-to-servers>Copy id_rsa.pub to servers</h3><h4 id=hadoop102>Hadoop102</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop102
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop103
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop104
</span></span></code></pre></div><h4 id=hadoop103>Hadoop103</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop103 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop102
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop103 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop103
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop103 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop104
</span></span></code></pre></div><h4 id=hadoop104>Hadoop104</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop104 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop102
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop104 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop103
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop104 .ssh<span style=color:#f92672>]</span>$ ssh-copy-id hadoop104
</span></span></code></pre></div><h2 id=configure-workers>Configure workers</h2><p>When the start and stop shell scripts were ran, the file <code>etc/hadoop/workers</code> will be analyzed. The content on this files are address of all servers. These scripts would run HDFS and YARN automatically on servers.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 hadoop<span style=color:#f92672>]</span>$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers
</span></span></code></pre></div><p>edit content (do not leave space on content):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>hadoop102
</span></span><span style=display:flex><span>hadoop103
</span></span><span style=display:flex><span>hadoop104
</span></span></code></pre></div><p>Sync <code>workers</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 hadoop<span style=color:#f92672>]</span>$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/
</span></span></code></pre></div><h2 id=operate-hdfs-and-yarn>Operate HDFS and YARN</h2><p><code>Attention!</code> The NameNode should be formatted only and only if the HDFS is firstlly started.</p><blockquote><p>Once the HDFS was started and NameNode need to be formatted again, a new cluster id would be created. Then the new cluster id on NameNode and old cluster id on DataNode would be different so that it would report an erro. Hence, if the NameNode need be formatted again, the processing of NameNode and DataNode should be stopped before. Besides, the dictionary <code>data</code> and <code>logs</code> should be deleted before.</p></blockquote><h3 id=start-and-stop-hdfs>Start and stop HDFS</h3><p>start HDFS.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 hadoop-3.1.3<span style=color:#f92672>]</span>$ sbin/start-dfs.sh
</span></span></code></pre></div><p>Enter http://hadoop102:9870 on web to check storage information on HDFS.</p><p>stop HDFS.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 hadoop-3.1.3<span style=color:#f92672>]</span>$ sbin/stop-dfs.sh
</span></span></code></pre></div><h3 id=start-and-stop-yarn>Start and stop YARN</h3><p>Start YARN.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop103 hadoop-3.1.3<span style=color:#f92672>]</span>$ sbin/start-yarn.sh
</span></span></code></pre></div><p>Enter http://hadoop103:8088 on web to check Jobs on YARN.</p><p>stop Yarn.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop103 hadoop-3.1.3<span style=color:#f92672>]</span>$ sbin/stop-yarn.sh
</span></span></code></pre></div><h2 id=write-and-xsync-shell-scripts>Write and <code>xsync</code> shell scripts</h2><h3 id=jps_clustersh>jps_cluster.sh</h3><p>Write a script that check Java&rsquo;s processings on servers of a cluster: <code>jps_cluster.sh</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ cd /home/jihang/bin
</span></span><span style=display:flex><span><span style=color:#75715e># [jihang@hadoop102 ~]$ touch jps_cluster.sh</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ sudo vim jps_cluster.sh
</span></span></code></pre></div><p>add content:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> host in hadoop102 hadoop103 hadoop104
</span></span><span style=display:flex><span><span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>        echo <span style=color:#f92672>===============</span> $host <span style=color:#f92672>===============</span>
</span></span><span style=display:flex><span>        ssh $host jps $@ | grep -v Jps
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>Authorize <code>jps_cluster.sh</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 bin<span style=color:#f92672>]</span>$ chmod +x jps_cluster
</span></span></code></pre></div><p>Sync shell scripts.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ xsync /home/jihang/bin/
</span></span></code></pre></div><h3 id=hadoop_clustersh>hadoop_cluster.sh</h3><p>Write a script that start and stop HDFS, YARN and historyserver on a cluster: <code>hadoop_cluster.sh</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ cd /home/jihang/bin
</span></span><span style=display:flex><span><span style=color:#75715e># [jihang@hadoop102 ~]$ touch hadoop_cluster.sh</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ sudo vim hadoop_cluster.sh
</span></span></code></pre></div><p>add content:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>if</span> <span style=color:#f92672>[</span> $# -lt <span style=color:#ae81ff>1</span> <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;No Args Input...&#34;</span>
</span></span><span style=display:flex><span>    exit ;
</span></span><span style=display:flex><span><span style=color:#66d9ef>fi</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>case</span> $1 in
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;start&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; =================== 启动 hadoop集群 ===================&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 启动 hdfs ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop102 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&#34;</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 启动 yarn ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop103 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&#34;</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 启动 historyserver ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop102 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&#34;</span>
</span></span><span style=display:flex><span>;;
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;stop&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; =================== 关闭 hadoop集群 ===================&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 关闭 historyserver ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop102 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&#34;</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 关闭 yarn ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop103 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&#34;</span>
</span></span><span style=display:flex><span>        echo <span style=color:#e6db74>&#34; --------------- 关闭 hdfs ---------------&#34;</span>
</span></span><span style=display:flex><span>        ssh hadoop102 <span style=color:#e6db74>&#34;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&#34;</span>
</span></span><span style=display:flex><span>;;
</span></span><span style=display:flex><span>*<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;Input Args Error...&#34;</span>
</span></span><span style=display:flex><span>;;
</span></span><span style=display:flex><span><span style=color:#66d9ef>esac</span>
</span></span></code></pre></div><p>Authorize myhdoop.sh.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 bin<span style=color:#f92672>]</span>$ chmod +x hadoop_cluster.sh
</span></span></code></pre></div><p>Sync shell scripts.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>[</span>jihang@hadoop102 ~<span style=color:#f92672>]</span>$ xsync /home/jihang/bin/
</span></span></code></pre></div><h2 id=run-example-wordcount>Run example <code>wordcount</code></h2><p>Run the official example <code>wordcount</code> on cluster.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount file:///wcinput file:///wcoutput
</span></span></code></pre></div><p><code>/</code> in <code>file:///wcinput</code> and <code>file:///wcoutput</code> means root path in HDFS, which is configurated by <code>core-site.xml</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>&lt;!-- 指定NameNode的地址 --&gt;
</span></span><span style=display:flex><span>&lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;fs.defaultFS&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;hdfs://hadoop102:9820&lt;/value&gt;
</span></span><span style=display:flex><span>&lt;/property&gt;
</span></span></code></pre></div><p><code>file:///</code> equals <code>hdfs://hadoop102:9820/</code>. So, The command below is equivalent to command above.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount hdfs://hadoop102:9820/wcinput hdfs://hadoop102:9820/wcoutput
</span></span></code></pre></div></section><div class=paginator><a class=prev href=https://10iA0.github.io/blog/wsl-introduction/><span>&larr;&nbsp;&nbsp;</span><span>WSL Introduction</span></a>
<a class=next href=https://10iA0.github.io/blog/hadoop-appendix/><span>Hadoop-Appendix</span><span>&nbsp;&nbsp;&rarr;</span></a></div><div class=related-resources><h3>Related Resources</h3><nav><ul><li><a href=/blog/hadoop-appendix/>Hadoop-Appendix</a></li><li><a href=/blog/hadoop-configurate-a-cluster/>Hadoop-Configurate a cluster (Core, HDFS, MapReduce and YARN)</a></li><li><a href=/blog/hadoop-running-a-cluster-write-sync-scripts/>Hadoop-Running a cluster-Write sync scripts</a></li><li><a href=/blog/hadoop-preparation-on-servers/>Hadoop-Preparation on servers</a></li></ul></nav><nav><ul><li><a href=/blog/hadoop-appendix/>Hadoop-Appendix</a></li><li><a href=/blog/hadoop-configurate-a-cluster/>Hadoop-Configurate a cluster (Core, HDFS, MapReduce and YARN)</a></li><li><a href=/blog/hadoop-running-a-cluster-write-sync-scripts/>Hadoop-Running a cluster-Write sync scripts</a></li><li><a href=/blog/hadoop-preparation-on-servers/>Hadoop-Preparation on servers</a></li></ul></nav></div></article></div><footer class=footer><p>&copy; 2022 <a href=https://10iA0.github.io>Horseman, pass by.</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"/><polyline points="5 12 12 5 19 12"/></svg></a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>